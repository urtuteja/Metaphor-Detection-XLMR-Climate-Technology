{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9b9485a",
   "metadata": {},
   "source": [
    "## Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07fc60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2be6b24",
   "metadata": {},
   "source": [
    "## Flattening the original Femke's dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1acf17",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"/Users/urtejakubauskaite/Desktop/Metaphors/Final project/Femke's Dataset/Final table OSF source domains.xlsx\",\n",
    "                   skiprows = 3, header = None)\n",
    "\n",
    "all_texts = df.values.flatten()\n",
    "all_texts = [str(text).strip() for text in all_texts if pd.notna(text) and str(text).strip() != '']\n",
    "\n",
    "df_output = pd.DataFrame(all_texts, columns = [\"Text\"])\n",
    "df_output.to_excel(\"Flattened_Femke's_dataset.xlsx\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06af3c28",
   "metadata": {},
   "source": [
    "## Tokenizing selected rows\n",
    "\n",
    "After receiving the document *Flattened_Femke's_dataset*, I manually deleted rows that did not contain any context, such as the plain phrase *buffer pool*, as well as rows that included metaphors consisting of three or more words. Afterwards, I created a document with tokenized sentences in the same format as the other documents used for testing the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a212b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"Flattened_Femke's_dataset_max_two_with_context.xlsx\", \n",
    "                   header = None)\n",
    "\n",
    "token_data = []\n",
    "\n",
    "for sent_id, row in df.iterrows():\n",
    "    sentence = str(row[0])\n",
    "    doc = nlp(sentence)\n",
    "    \n",
    "    for token_id, token in enumerate(doc):\n",
    "        token_data.append({\"sent_id\": sent_id,\n",
    "                           \"token_id\": token_id,\n",
    "                           \"token_text\": token.text,\n",
    "                           \"pos\": token.pos_,\n",
    "                           \"FINAL\": \"\"})\n",
    "\n",
    "token_df = pd.DataFrame(token_data)\n",
    "\n",
    "token_df.to_excel(\"tokenized_femke_texts.xlsx\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa74568",
   "metadata": {},
   "source": [
    "## Correcting the file to its final version\n",
    "\n",
    "After manually marking metaphors in *tokenized_femke_texts* based on Femke's bold-text annotations, I noticed a mistake in the file: the token count restarted from zero in every sentence. I corrected this issue and also added \"0\"s in the *FINAL* column where cells were empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a49782",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"tokenized_femke_texts.xlsx\")\n",
    "\n",
    "# Fixing the token count\n",
    "df[\"token_id\"] = range(len(df))\n",
    "\n",
    "# Replacing empty values with \"0\"\n",
    "df[\"FINAL\"] = df[\"FINAL\"].fillna(\"0\").replace(\"\", \"0\")\n",
    "\n",
    "df.to_excel(\"tokenized_femke_texts_final.xlsx\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f60412",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
